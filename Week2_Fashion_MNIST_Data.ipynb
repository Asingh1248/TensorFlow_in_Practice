{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week2_Fashion_MNIST_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOKMAUsIrlK/zoMdo9fzMRq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asingh1248/TensorFlow_in_Practice/blob/master/Week2_Fashion_MNIST_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVM-2qlgXLt6",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AwHmrOZXNZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "beac19ed-abbf-4f96-ca4c-7418b52c0026"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amXpD9m-XQmE",
        "colab_type": "text"
      },
      "source": [
        "# Providing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LemFKcqoXPxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "878199e8-00c1-412a-af5c-146daa3662ca"
      },
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEOCGddGXeQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explaination \n",
        "# 1. tf.keras.dataset... = Data is available in keras.dataset API\n",
        "# 2. (train_data,label)(test_dt,label) = mnist.load_data () ==  load_data on this object will give you two sets of two lists it has cloting items and labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfwh5YzAYOJk",
        "colab_type": "text"
      },
      "source": [
        "# Ploting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx0WDRzfYK5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(linewidth=200)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(training_images[50])\n",
        "print(training_labels[50])\n",
        "print(training_images[50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK_cpnXWYjir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explaination\n",
        "# 1. lables is printed for the train_ input\n",
        "# 2. Actual 28 x28 array with different pixel value ranging from 0 - 255\n",
        "# 3. Actual images in printed "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhxcHm5PZC2y",
        "colab_type": "text"
      },
      "source": [
        "# Normalizing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdWnTvq5ZLup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maoqyIKmZaqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explaination (Exercise Try removing 255.0)\n",
        "# 1 : Since the values is from 0 - 255 , in Neural network is better that we treat the values from 0-1\n",
        "# 2 : Feature of Python to divide whole list by 255 to get values from 0 to 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6zFZGkwDQ5e",
        "colab_type": "text"
      },
      "source": [
        "# CallBack for Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjeXmKugDUKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epoch,logs={}):\n",
        "    if(logs.get('accuracy')>0.6): # 'loss'\n",
        "      print(\"\\n Reached 60% accuracy so cancelling training !\")\n",
        "      self.model.stop_training=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u3G_t2JZ1AY",
        "colab_type": "text"
      },
      "source": [
        "# Building the ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eupdYCoZxbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = myCallback()\n",
        "model= tf.keras.Sequential(\n",
        "    \n",
        "     [\n",
        "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "      tf.keras.layers.Dense(128, activation= tf.nn.relu),\n",
        "      tf.keras.layers.Dense(10,activation = tf.nn.softmax)\n",
        "      ]\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LLiNRxDah8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explaination (Excerise :Try commenting Flatten , Increase No of units in Dense , Add another Dense layer, Change the no of units in output layers ) \n",
        "# 1 : Flatten # : Images were a square, when you printed them out? \n",
        "               # Flatten just takes that square and turns it into a 1 dimensional set. \n",
        "                \n",
        "# 2 i): Dense(128) Hidden layer with 128 neurons\n",
        "#  ii): Dense(activation='relu') Hidden layers needs it to tell them what to do \n",
        "#             relu effectively means \"If X>0 return X, else return 0\" -- so what it does it it only passes values 0 or greater to the next layer in the network.                               \n",
        "\n",
        "# 3 Dense(activation = 'softmax') : Softmax takes a set of values, and effectively picks the biggest one, so, \n",
        "#      for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], \n",
        "#    it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding! \n",
        "\n",
        "# Actual Undertsnading comes in classification "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjpYYVdLcmW_",
        "colab_type": "text"
      },
      "source": [
        "# Compiling the ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7udd-YAcpFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u43G9qzcrx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explainations : sdg was used in Liner Regression , loss = 'mean _squared _error' in previous section\n",
        "# Adam :\n",
        "# metrics=['accuracy']\n",
        "# loss = 'sparse_categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "codLo1Gqc1Q7",
        "colab_type": "text"
      },
      "source": [
        "# Fitting the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoefilXbc30T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(training_images, training_labels, epochs=5,callbacks=[callbacks])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKBiwnG2c_AU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imp Exercise 6 and 8:.\n",
        " \n",
        "# Try 15 epochs -- you'll probably get a model with a much better loss than the one with 5 \n",
        "# Try 30 epochs -- you might see the loss value stops decreasing, and sometimes increases. \n",
        "# This is a side effect of something called 'overfitting' which you can learn about [somewhere] and \n",
        "# it's something you need to keep an eye out for when training neural networks. \n",
        "# There's no point in wasting your time training if you aren't improving your loss, right! :)\n",
        "\n",
        "### CALLBACK FUNCTION WHERE ME Mention after this of accucracy stop the iteration (epochs)\n",
        "\n",
        "# Step 1 : Create class (tf.keras.callbacks.Callback) , function (epochs,logs{}) ,logs.get()>0.6 model.stop\n",
        "# Step 2:  Instatiate class before building the model\n",
        "# Step 3: add a parameter [callbacks]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tllzm0lceHOd",
        "colab_type": "text"
      },
      "source": [
        "# Predicting and Classification the Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O2kPvd5eXXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(classifications[10])\n",
        "print(test_labels[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDuvRIQTewx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explainations :\n",
        "# 1 . predict(test_images) and classifications[0] returns list having probability for all the all the labels from 0 to 9\n",
        "# 2. test_labels[0]  : select is images is what if 9 then ankle boots why 9 numbers why not ankle boots (Figured out)?\n",
        "\n",
        "# Note : # Dense(activation = 'softmax') for output layers comes into picture since softmax generates an lsit with probablity for all 10 images and selects biggest one \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}